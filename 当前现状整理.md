##### 数据集调研 
公开数据集
 - **criteo**
   |模型|AUC|Logloss|
   |--|--|--|
   |DeepLight|0.8123|0.4395|
   |NormDNN|0.8107|NA|
   |FAT-DeepFFM|0.8104|0.4416|
   |xDeepFM|0.8052|0.4418| 
 
 - **Avazu**
   |模型|AUC|Logloss|
   |--|--|--|
   |Fi-GNN|0.812|0.3817|
   |DeepLight|0.7897|0.3748|
   |FGCNN+IPNN|0.7883|0.3746|
    
 - **Movie Lens 20M**   
   |模型|AUC|F1|
   |--|--|--|
   |KGCN-sum|0.978|0.932|
   |KNI|0.9704|NA|
   |DIN + Dice Activation|0.7348|NA|
   |DIN|0.7337|NA|
   |DeepFM|0.7324|NA|
 
 - **Amazon product reviews**
   |模型|AUC|
   |--|--|
   |DIN + Dice Activation|0.8871|
   |DIN|0.8818|
   |DeepFM|0.8683|
   |Wide & Deep|0.8637|
   
##### 参考模型比较
 - #### **xDeepFM**
    - **主要创新点**：
        1. 提出xDeepFM,同时implicit和explicit地学习高阶特征，不需手动制作特征
        2. 提出CIN， 明确地学习high-order特征
    
    - **相关工作**：
        1. Embedding 
        2. Implicit High-order Interactions
            - 对高阶特征的交互没有明确定义，而是通过映射到Embedding Layer，通过拼接或其他方式交互
            - 例如FNN，Deep Crossing, Wide&Deep, PNN, DeepFM 
        3. Explicit High-order Interactions
            - Deep&CrossNet,每一层表示一阶特征，k-layer表示k阶交互特征
    - **主要内容**
        - Compressed Interaction Network(CIN)
            1. 交互以vector为单位，不以bit为单位。
            2. high-order特征显示学习
            3. 计算复杂度不指数增长
        - CIN与DNN两路合并
     
    - **实验数据集**：Criteo, Dianping(自有), BingNews（自有）
    - **实验内容**：
        1. CIN Compressed Interaction Network（单独的深度网络）
        2. 与Deep部分合体实验
       
 - #### **NormDNN**
     - **主要创新点**：
        1. 新的Layer Normalization方法， variance-only
        2. 在Embedding和MLP两部分试验了不同的Normalization方法，三个数据集
        3. 提出NormDNN，对于numerical feature，categorical feature和mlp采用特定的Normalization方法，比xDeepFM效果好
        4. 在DeepFM和xDeepFM上应用不同的normal策略，证实有提升效果
        5. 研究normalzaition有效的原因：variance
     - **相关工作**：
         - Normalization
         - NN in CTR
     - **实验数据集**：Criteo，Malware，Avazu
     - **实验内容**
         1. 不同normal方法只应用于feature embedding
         2. 不同normal方法只应用于MLP
         3. 均应用
         4. categorical feature和numerical feature所需normal方法是否不同
         5. 有没有一个最佳组合
         6. DeepFM和xDeepFM应用normal的效果
 
 - #### **FAT DeepFFM**
    - **主要创新点**
        1. 提出新的模型，对DeepFFM进行改进，在DeepFFM基础上引入field-aware attention机制（基于SENet改进的CENet）
        2. 比较了两种不同的attention机制（在feature交互之前 or 在交叉特征上进行attention）
        3. 两个数据集进行试验证明FAT-DeepFFM有效性
    - **相关工作**
        1. FM和FFM
        2. Deep Learning CTR
        3. CTR中的attention，AFM，DIN
    - **主要内容**
        1. DeepFFM：对每个field，转换成一个k * n的Embedding矩阵EM[i]，k为Embedding vector size，n为field总数，即将每个field i表示为和所有其他field交互的向量, EM[i,j]即为Field i和j交互的Embedding。然后通过內积计算获得interaction layer
        2. SENet，先将每个EM进行1x1 conv，然后计算attention，然后作为权重和每个EM矩阵进行点乘
        3. SENet和DeepFFM融合，在Interaction产生之前进行Attention，替换原有EM
    - **数据集**：Criteo, Avazu
    - **实验内容**
        1. 模型是否有效
        2. Attention机制放置的位置
        3. 不同特征交互的方法：Inner Product 或 Hadamard-product
       

 - #### **Deep Light**
     - **主要创新点**
        1. High Quality：提出DeepFwFM，利用Field importance思想，FwFM改进low-order特征交互。比xDeepFM效果好且复杂度很低
        2. Low latency：
            1. 多余参数prune
            2. 移除 weak field pairs
        3. low consumption：EMbedding压缩
        4. 实现 46x 加速 on criteo， 27x加速 on Avazu
     - **相关工作**
         - FM，DeepFM，xDeepFM
     - **主要内容**
         - DeepFwFM：
             - 1. 和DeepFM类似，不过在FM到output时，加入Field Weight，所以比DeepFM效果更好
             - 2. 不考虑3阶以上的特征交互，所以比XDeepFM快很多
         - DeepLight:
            1. 参数剪枝：通过随机将参数置为0来实现，参数矩阵稀疏，采用CRS来计算乘法
            2. Embedding Layer裁剪
     - **数据集**：Criteo，Avazu
     - **实验内容**：
        1. DeepFwFM效果
        2. DeepLight：
            1. 不同裁剪率的AUC变化
            2. 不同部分：DNN，field matrix，Embedding vector裁剪的比较
            3. 裁剪后的模型和其他Sparse Model对比

 - #### **DIN**
     - **主要创新点**
         1. 其他模型对于用户的历史行为序列，映射到一个固定长度的Embedding向量，没有充分利用。提出DIN，使用一个local activation unit来学习user interests
         2. 应对工业级的深度网络，提出两个优化方法：
             1. mini-batch的regularizer：降低参数量
             2. data自适应的激活函数， PReLU，根据数据分布调整
         3. 公开数据和Ali数据实验
    - **主要内容**
        - 对用户的历史行为序列，设立了权重进行加权求和，权重由attention的方式进行学习，结构较为清晰，激活函数后的标准化有些trick
        - mini-batch 的regularizer：
            - 因为CTR模型的主要参数都在Embedding dictionary上，优化的方式是估算正则项，只计算batch中存在的特征
    - **数据集**：Amazon, MovieLens20M, Alibaba（自有）
    - **实验内容**：
        1. 在Amazon, MovieLens数据集上进行多模型比较
        2. 在alibaba数据集上，存在比较严重过拟合问题，比较dropout， Filter等方法，和MiniBatch regularizer进行比较
        3. Alibaba数据集上各模型的效果
        4. AB test效果
        5. DIN可视化，展现Interest效果
 
 - #### **DIEN**
     - **创新点**
         1. 提出新model，刻画interest evolve过程
         2. 设计interest extract layer, 并对GRU的hidden state进行优化，额外加入loss用以监督学习
         3. 设计interest evolving layer
    
     - **主要内容**
        1. Interest Extractor Layer
            - 本质是利用GRU来进行表示，但是加入了Auxiliary loss，来增加序列中的监督信息，使用t+1的label和t的hidden去关联，
        2. Interest Evolving Layer
            - 提出基于Attention的GRU单元，AUGRU，刻画兴趣的演进关系，
     - **数据集**
        1. Amazon
        2. alibaba自有数据
     - **实验内容**
        1.  比较各模型效果
        2.  分析Auxiliary loss的效果
        3.  分析AUGRU效果
        4.  可视化Interest

- #### **Fi-GNN**
    - **创新点**
        1. 多个field的特征构建成图结构
        2. Fi-GNN，提取图结构特征
        3. 两个数据集上进行实验
    - **相关工作**
        1. ctr中的特征交互组合
        2. 图神经网络
            - 从图中构建序列，预训练Embedding： DeepWalk， LINE，node2vec
            - Gated GNN， GCN，GraphSAGE， GAT 
    - **主要内容**
        - FiGNN
            1. 状态聚合：邻接矩阵设成带权值
            2. 转移函数：设为Edge-wise，为了降低参数量，进行矩阵分解，类似FM的做法
            3. 状态更新：GRU模块来更新，或者加入残差机制
        - Attention Score Layer
            - 计算每个node的权值，用于加权计算最终的logit
    - **数据集**： Criteo， Avazu
    - **实验内容**：
        1. 模型比较
        2. Fi-GNN和普通GGNN的区别
        3. 不同model配置的影响
        4. 可解释性

- #### **FLEN**
    - **创新点**
        1. 利用多个模块，来从不同level提取特征（FM-based Field内部交互，MF-based field间交互，MLP layer 非线性特征）
    - **主要内容**
        - 主要是在浅层拼接了各种模块
    - **数据集**：Avazu, Meitu（自有）
    - **实验内容**
        - 模型性能
        - 参数规模
- #### **MIMN**
    - **创新点**
        1. 主要解决长用户序列分析的问题，降低资源消耗
        2. 分离出UIC模块，用于预先算好并保存用户最新的兴趣表征，降低整个即时预估服务的时延
        3. MIMN model，借鉴NTM中memory network思路，加入memory utilization regularizaton和memory induction unit, 更高效的建模用户行为序列
     
    - **数据集**： Amazon, Taobao, Alibaba（自有）
      
